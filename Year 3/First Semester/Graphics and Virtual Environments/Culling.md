(Mostly copying from the given notes, cause i totally missed this topic until revising for the exam)

For most graphical scenes it is likely that there’s much more detail or content in the underlying model than is visible in any one rendered view; this is particularly true for interactive applications with dynamic content where objects and the viewpoint vary. There’s an obvious performance advantage to be had if you can work out which bits of the scene aren’t going to contribute to the rendered view (i.e. if you can’t see them, don’t waste time drawing them!) This might seem like an obvious thing to try to achieve, but it’s not trivial to achieve, since you have to make sure that the cost of excluding something from being rendered doesn’t exceed the cost of just rendering it. We’ll look at a number of ‘culling’ techniques designed to achieve this.
## Detail culling
The first, and most simple of these techniques is called ‘detail culling’. The idea here is to not bother drawing things that are simply too small to have a perceivable effect on the final scene. This could be a small object reasonably close up, or a huge one in the far distance. All we need to determine here is the size of the object when projected onto the view plane – this could just be a simple bit of trigonometry based on the bounding cuboid of the object (and culling out large objects in the distance is particularly satisfying, since spending a lot of effort drawing 1000s of textured polygons, only to find out that the result is a single pixel in the final image is obviously wasteful). This approach is particularly effective for moving scenes, since the viewer is unlikely to notice tiny bits of lost detail.

**tl;dr: Use trigonometry (and other basic math) to rule out objects that are so small they would barely even be visible, if at all. This is on the entire object, not on polygons!**
## Backface culling
It’s fairly obvious that for most objects you can’t see all of them all at the same time; the chances are that some bits of the object are facing you and are therefore visible, and that some parts are on the ‘other side’, hidden from view. This is certainly true for opaque convex objects, where the front facing surface hides the back surface; it’s a bit more complex for ‘open’ or concave objects (for example, you can see the back face of an open box). What this does mean though is that for a lot of things we could save time by simply not rendering the polygons that are facing away from the viewer. Figure 7 shows a simple model, first rendered with filled polygons, then with all its polygons visible, and finally with only those poylgons that have a surface normal facing the viewplane drawn. You can see there is a considerable saving in terms of the number of polygons shown; not surprisingly, somewhere in the region of 50% in this case. But how do you determine whether a polygon is facing towards or away from the viewer?

There are two considerations to take into account here; first, it has to be a cheap calculation (otherwise we might as well just draw all the polygons and let the z-buffer take care of hiding the ones behind), and second it has to ideally be something we can do early on in the rendering pipeline (otherwise, if we’ve done most of the hard work transforming a polygon, we might as well just draw it anyway and again let the z-buffer do the occlusion work for us).
![](Pasted%20image%2020240114180032.png)
**Figure:** A model of a sofa rendered (a) with filled polygons, (b) showing the wireframe of its polygonal mesh, and (c) showing only those polygons that face the viewer.

We can do a simple test on polygons in object-space by calculating the dot product of the view vector and the polygon’s surface normal, and looking at the sign of the result. If it’s negative, the polygon faces away from the viewer and can be discarded; if it’s positive, it’s facing towards us and should be drawn (we are essentially testing wether the polygon is angled more than 90 degrees away from perpendicular to the view vector). The down side of this approach is that it does require a bit of trigonometry (recall the dot product of two vectors will require a cosine calculation) – but modern hardware often has this calculation available as a GPU instruction, so this isn’t too painful.
![](Pasted%20image%2020240114180226.png)
There is one alternative to doing the calculation in world space, which is to look at the winding of the polygon as it is rendered onto the view plane; assuming that our model has a consistent polygon winding, then resulting polygons wound one way when projected on to the viewplane will be facing towards the viewer, and those wound the other will be facing away from the viewer.

**tl;dr: polygons that are "facing" away from the camera can be culled (subject to lots of conditions). You can do this by comparing the normal of the polygon to the camera's direction (more than 90 degrees diff means you're seeing the backface and can skip drawing) OR, assuming the polygon winding is consistent you can determine the facing using that.**
## Frustum Culling
The viewing frustum is a geometric representation of the volume visible to the virtual camera. Naturally, objects outside this volume will not be visible in the final image, so they are discarded. Often, objects lie on the boundary of the viewing frustum. These objects are cut into pieces along this boundary in a process called clipping, and the pieces that lie outside the frustum are discarded as there is no place to draw them. The various spatial enumeration techniques described in [Spatial Enumeration](Spatial%20Enumeration.md) make it possible to quickly determine which objects are candidates for culling.

## Occlusion Culling
Summary: We wanna skip rendering objects that are entirely occluded by other objects. We already solved the basic issue here with the Z-buffer (that is, objects behind will not be wrongly drawn in front) but we still have to entirely render that object before the Z-buffer rules it out, wasting loads of performance. Ideally we want something that acts like the Z-buffer but sooner in the pipeline, letting us entirely skip rendering occluded objects. The only method that really matters is:
### Portal Culling
It’s common, for example, that indoor scenes consist of enclosed spaces of one kind or another, with portals - doors and windows - through which other enclosed spaces are potentially visible.
![](Pasted%20image%2020240114181318.png)
From any given viewpoint, then, its likely that you can potentially see things in the current room, and in any room to which there is a direct line of sight through ‘portals’. The ‘portal culling’ approach works like roughly like this:
1. From the current viewpoint, identify any portals that are partly or wholly visible in the current view frustum.
2. For each portal, cast a rays against the perimeter of the portal into other rooms.
3. For each room that a ray enters, identify portals that are visible in the view frustum, and repeat the ray casting process.
4. Each room that a ray passes through has contents that are potentially visible from the original viewpoint; create a new viewing ‘frustum’ (the portal need not be square, so it’s not always technically a frustum) from the eyepoint that takes into account the frame of the portal, and repeat the process from Step 1 until all visible portals have been included.